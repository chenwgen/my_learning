{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 介绍\n",
    "基于神经网络的方法，在命名实体识别任务中非常流行和普遍。本文将使用一个例子解释CRF层是如何工作的。\n",
    "原文链接https://createmomo.github.io/2017/09/12/CRF_Layer_on_the_Top_of_BiLSTM_1/\n",
    "\n",
    "### 1.1 开始之前\n",
    "假设我们的数据集中有两类实体 —— 人名(Person)和机构名(Organization)，与之相对应有五类实体标签：\n",
    " - B-Person\n",
    " - I- Person\n",
    " - B-Organization\n",
    " - I-Organization\n",
    " - O\n",
    "\n",
    "同时，句子 $x$ 由五个词 $w_1$, $w_2$ , $w_3$ ,$w_4$ ,$w_5$ 组成，其中[ $w_1$,$w_2$ ]为人名类实体，[ $w_3$ ]为机构类实体，其他词标签为\"O\"。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 BiLSTM-CRF模型\n",
    "简单介绍一下这个模型。<br/>\n",
    "&ensp;&ensp;如下图所示：<br/>\n",
    "(1) 句子 $x$ 中每一个词都由字嵌入或词嵌入构成的向量。其中，字嵌入是随机初始化的，词嵌入由预训练好的词嵌入文件得到。所有的嵌入在训练过程中都会调整到最优。<br/>\n",
    "(2) BiLSTM-CRF模型的输入是词嵌入，输出是句子 $x$ 中每个词的标签。\n",
    "![BiLSTM-CRF_struct](http://localhost:9999/files/my_learning/pic/CRF-LAYER-1-v2.png)\n",
    "尽管不需要详细了解BiLSTM层的原理，但是为了更容易理解CRF层，我们需要知道BiLSTM的输出层。\n",
    "![BiLSTM_struct](http://localhost:9999/files/my_learning/pic/CRF-LAYER-2-v2.png)\n",
    "&ensp;&ensp;&ensp;&ensp;如上图所示，BiLSTM层的输出是每个标签的预测分值，例如，对于词 $w_0$ ,BiLSTM层输出的是1.5 (B-Person), 0.9 (I-Person), 0.1 (B-Organization), 0.08 (I-Organization) 和 0.05 (O). 这些分值将作为CRF的输入。<br/>\n",
    "&ensp;&ensp;&ensp;&ensp;然后，BiLSTM层的所有预测分值会喂给CRF层。CRF层将选最高预测分值的标签序列作为结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 如果没有CRF层会怎样\n",
    "你也许已经发现了，即使没有CRF层，我们也可以训练一个BiLSTM命名实体识别模型，如下图所示:\n",
    "![BiLSTM_struct](http://localhost:9999/files/my_learning/pic/CRF-LAYER-3.jpg)\n",
    "由于BiLSTM的输出为词的每一个标签分值，我们可以挑选分值最高的一个作为该词的标签。比如，对于单元 $w_0$ ,\"B-Person\"有最高分值(1.5)，因此可以挑选\"B-Person\"作为 $w_0$ 的预测标签。同理，可以得到 $w_1$ —\"I-Person\"，$w_2$ —\"O\" ，$w_3$ —\"B-Organization\"，$w_4$ —\"O\"。<br/>\n",
    "\n",
    "虽然我们可以得到句子 $x$ 中每个词汇的正确标签，但是不能保证标签每次都预测正确。比如下图所示：\n",
    "![BiLSTM_struct](http://localhost:9999/files/my_learning/pic/CRF-LAYER-4.jpg)\n",
    "显然，标签序列\"I-Organization I-Person\" 和 \"I-Organization I-Person\"是错误的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 CRF层能从训练数据中获得约束性的规则\n",
    "CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合理的。在训练过程中，CRF层可以从训练数据中自动学习到这些约束。<br/>\n",
    "这些约束可能是:<br/>\n",
    "- 句子第一个词应该以标签\"B-\" 或 \"O\"开始，而不是\"I-\"。\n",
    "- \"B-label1 I-label2 I-label3 I-…\"这个模式,label1, label2, label3应该属于同一类实体。例如，\"B-Person I-Person\" 是合理的序列, 但是\"B-Person I-Organization\" 是无效标签序列。\n",
    "- 标签序列\"O I-label\" 是不合理的.实体标签的首个标签应该是 \"B-\" ，而非 \"I-\", 换句话说,有效的标签序列应该是\"O B-label\"。\n",
    "\n",
    "有了这些约束，标签序列预测中不合理的序列出现的概率将会大大降低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. CRF层\n",
    "CRF层的损失函数有两类得分。这两类得分是CRF层的关键概念。\n",
    "\n",
    "### 2.1 发射概率 (Emission score)\n",
    "第一类是发射概率。这些概率来自BiLSTM。比如，如下图所示，标签为\"B-Person\"的词 $w_0$ 的发射概率是1.5 。\n",
    "![BiLSTM_struct](http://localhost:9999/files/my_learning/pic/CRF-LAYER-2-v3.png)\n",
    "方便起见，我们用索引值来表示各个实体标签，对应关系如下：<br/>\n",
    "\n",
    "Lable|Index\n",
    "--- | ---\n",
    "B-Person|0\n",
    "I-Person|1\n",
    "B-Organization|2\n",
    "I-Organization|3\n",
    "O|4\n",
    "\n",
    "我们使用 $x_{iy_j}$ 表示发射概率。$i$ 是词的索引，$y_j$ 是标签的索引。比如，根据上图所示，$x_{i=1,y_i=2}=x_{w_1,B-Organization}=0.1$ ，表示 $w_1$ 的标签为\"B-Organization\"的发射概率是0.1 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 转移概率 (Transition score)\n",
    "我们使用 $t_{y_i y_j}$ 表示转移概率。比如: $t_{B-Person,I-Person}=0.9$ 表示 $B-Person \\rightarrow I-Person$ 的概率是0.9。因此，有转移概率矩阵来保存所有标签的转移概率。<br/>\n",
    "为了使转移概率矩阵更具鲁棒性，我们额外增加两个标签:\"START\"和\"END\"，\"START\"代表句子的开始位置，而非第一个词。同理，\"END\"代表句子的结束位置.<br/>\n",
    "下面是一个转移概率矩阵的例子，包括\"START\"和\"END\"标签。\n",
    "\n",
    "|START|B-Person|I-Person|B-Organization|I-Organization|O|END\n",
    ":-:|:-:|:-:|:-:|:-:|:-:|:-:\n",
    "START|0|0.8|0.007|0.7|0.0008|0.9|0.08\n",
    "B-Person|0|0.6|0.9|0.2|0.0006|0.6|0.009\n",
    "I-Person|-1|0.5|0.53|0.55|0.0003|0.85|0.008\n",
    "B-Organization|0.9|0.5|0.0003|0.25|0.8|0.77|0.006\n",
    "I-Organization|-0.9|0.45|0.007|0.7|0.65|0.76|0.2\n",
    "O|0|0.65|0.0007|0.7|0.0008|0.9|0.08\n",
    "END|0|0|0|0|0|0|0\n",
    "\n",
    "可以发现，转移概率矩阵已经学到了一些有用的约束:<br/>\n",
    "- 句子的第一个词应该以标签\"B-\" 或 \"O\"开始，而不是以标签\"I-\"开始(从\"START\"到\"I-Person\"或\"I-Organization\"的转移概率很低)。\n",
    "- \"B-label1 I-label2 I-label3 I-…\"这个形式,label1, label2, label3应该属于同一类实体。比如，\"B-Person I-Person\"是合理的，但是\"B-Person I-Organization\"是不合理的. (比如, \"B-Organization\"到\"I-Person\"的转移概率只有0.0003，比其他都低很多。)\n",
    "- \"O I-label\"是不合理的。实体标签的首个标签应该以\"B-“开始，而不是\"I-\"。换句话说，有效的标签序列形式是\"O B-label\"(例如，转移概率$t_{O,I-Person}$ 的值很低)。\n",
    "\n",
    "你可能会问转移概率矩阵是怎么得到的。实际上，转移概率矩阵是BiLSTM-CRF模型的参数。在训练模型之前，需要初始化转移概率矩阵，每个元素都是随机值。在进行模型训练时，这些随机值会自动更新，即CRF层会自动学习这些约束。我们不需要人工设定转移概率矩阵。随着训练迭代次数的增加，转移概率分值会越来越合理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 CRF损失函数\n",
    "CRF损失函数由真实路径分值和所有可能路径的总分组成。真实路径分值是所有可能路径的最高分值。比如，如果我们的数据集的标签如下表所示:\n",
    "\n",
    "Lable|Index\n",
    "--- | ---\n",
    "B-Person|0\n",
    "I-Person|1\n",
    "B-Organization|2\n",
    "I-Organization|3\n",
    "O|4\n",
    "START|5\n",
    "END|6\n",
    "\n",
    "一个句子有5个词语，可能的路径为:\n",
    "- 1) START B-Person B-Person B-Person B-Person B-Person END\n",
    "- 2) START B-Person I-Person B-Person B-Person B-Person END\n",
    "- ......\n",
    "- 10)**START B-Person I-Person O B-Organization O END**\n",
    "- ......\n",
    "- N) O O O O O O O\n",
    "\n",
    "假定每个可能的路径分值为 $P_i$，总共有N种可能路径。所有路径的总分为 $P_{total} = P_1 + P_2 +...+ P_N = e^{S_1} + e^{S_2}+...+ e^{S_N} $，$e$ 是数学常值 $e$ 。<br/>\n",
    "如果假定第10条路径是真实路径，即第10条路径是训练数据提供的黄金标准标签。分值 $P_{10}$ 应该是所有可能路径中所占比例最大的一个。<br/>\n",
    "给定以下表示损失函数的等式，在训练过程中，BiLSTM-CRF模型会不断更新参数，使真实路径的分值比例不断变大。\n",
    "> $ \\text{LossFunction } = \\frac { P_{RealPath} } { P _ {1} + P _ {2} + \\ldots + P_N } $\n",
    "\n",
    "现在的问题是:\n",
    "- 怎样定义一条路径的分值\n",
    "- 怎样计算所有可能路径的总分\n",
    "- 当我们计算总分时，是否必须列出所有可能路径。(答案是NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 真实路径分值\n",
    "在2.3节中，我们假定每个可能的路径分值为 $P_i$，总共有N种可能路径。所有路径的总分为 $P_{total} = P_1 + P_2 +...+ P_N = e^{S_1} + e^{S_2}+...+ e^{S_N} $，$e$ 是数学常值 $e$ 。<br/>\n",
    "显然，在所有可能路径中，必定有一条路径是真实路径。例如，1.2节中句子的真实路径是\"START B-Person I-Person O B-Organization O END\"，其他路径都是错误的，比如\"START B-Person B-Organization O I-Person I-Person B-Person\"。$e^{S_i}$ 是第 $i$ 条路径的分值。<br/>\n",
    "在训练过程中，CRF损失函数只需要两个分值:真实路径的分值和所有路径的总分。真实路径在所有路径中的分值占比会越来越大。<br/>\n",
    "计算真实路径的分值 $e^{S_i} $ 是很直接的。这里我们关注 $S_i$ 计算。<br/>\n",
    "用之前的真实路径\"**START B-Person I-Person O B-Organization O END**\"作为例子:\n",
    "- 一个句子有5个词汇: $w_1,w_2,w_3,w_4,w_5$ 。\n",
    "- 增加两个额外的词汇作为句子的开始和结束: $w_0,w_6$ 。\n",
    "- $S_i$ 包括两个部分: $S_i=EmissionScore+TranstionScore$ 。\n",
    "\n",
    "#### 2.4.1 发射概率(EmissionScore)\n",
    "$ EmissionScore=x_{0,START}+x_{1,B-Person}+x_{2,I-Person}+x_{3,O}+x_{4,B-Organization}+x_{5,O}+x_{6,END} $\n",
    "- $x_{index,label}$ 表示第 $index$ 个词汇的标签是 $label$ 。\n",
    "- $x_{1,B-Person},x_{2,I-Person},x_{3,O},x_{4,B-Organization},x_{5,O}$ 这些分值都是前一层BiLSTM的输出值。\n",
    "- $x_{0,START}和x_{6,END}$ 可以设置为0。\n",
    "\n",
    "#### 2.4.2 转移概率(Transition Score)\n",
    "$TransitionScore=t_{START\\rightarrow B-Person}+t_{B-Person\\rightarrow I-Person}+t_{I-Person\\rightarrow O}+t_{O\\rightarrow B-Organization}+t_{B-Organization\\rightarrow O}+t_{O\\rightarrow END}$\n",
    "- $t_{label1\\rightarrow label2}$ 是 $label1$ 到 $label2$ 的转移概率。\n",
    "- 这些分值来自CRF层，即转移概率是CRF层的参数。\n",
    "\n",
    "总结一下，现在我们可以计算 $S_i$ 和 $e^{S_i}$。下一步是怎样计算所有可能路径的总分。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 所有可能路径的总分\n",
    "在上一节中，我们学习了一个标签路径的分值 $e^{S_i}$ 。至此，我们还有一个问题需要解决，即如何获得所有路径的总分($P_{total}=P_1+P_2+...+P_N=e^{S_1}+e^{S_2}+...+e^{S_N}$)。<br/>\n",
    "最简单的方式是:列举出所有可能的路径，然后对其分值求和。你可以这样做，但是不是很高效，训练时间会难以忍受。<br/>\n",
    "在浏览以下内容前，建议你拿出纸和笔，跟着以下小例子的计算步骤，这样更有助于你理解算法的细节。\n",
    "#### 2.5.1 第1步:回顾CRF的损失函数\n",
    "在2.3节中，定义CRF的损失函数为:\n",
    "> $ \\text{LossFunction } = \\frac { P_{RealPath} } { P _ {1} + P _ {2} + \\ldots + P_N } $\n",
    "\n",
    "现在变为log损失函数:\n",
    "> $ \\text{LogLossFunction } =log \\frac { P_{RealPath} } { P _ {1} + P _ {2} + \\ldots + P_N } $\n",
    "\n",
    "在训练时，我们的目标通常是最小化损失函数，所以增加了负号:\n",
    "> $\\begin{split}\n",
    "\\text{LogLossFunction } \\\\\n",
    " &= -log \\frac { P_{RealPath} } { P _ {1} + P _ {2} + \\ldots + P_N } \\\\\n",
    " &= -log \\frac { e^{S_{RealPath}} } { e^{ S_ {1}} + e^{ S_ {2}} + \\ldots + e^{S_{N}} } \\\\\n",
    " &= -(log(e^{S_{RealPath}}\\quad)-log(e^{ S_ {1}} + e^{ S_ {2}} + \\ldots + e^{S_{N}})) \\\\\n",
    " &= -(S_{RealPath}-log(e^{ S_ {1}} + e^{ S_ {2}} + \\ldots + e^{S_{N}})) \\\\\n",
    " &= -(\\sum_{i=1}^Nx_{ij_i}+\\sum_{i=1}^{N-1}t_{y_iy_{i+1}}-log(e^{ S_ {1}} + e^{ S_ {2}} + \\ldots + e^{S_{N}}))\n",
    "\\end{split}$\n",
    "\n",
    "在之前的章节中，我们已经知道怎样计算真实路径的分值。现在需要找到有效的方法来计算 $log(e^{ S_ {1}} + e^{ S_ {2}} + \\ldots + e^{S_{N}})$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 第2步: 回顾发射概率和转移概率\n",
    "为了简单，我们假设句子只有3个词汇:\n",
    "> $ \\boldsymbol x=[w_0,w_1,w_2] $\n",
    "\n",
    "另外，数据集中只有2个标签:\n",
    "> $ \\boldsymbol labelSet=\\{l_1,l_2\\}$\n",
    "\n",
    "从Bi-LSTM层得到的发射概率如下:\n",
    "\n",
    "|$l_1$|$l_2$\n",
    ":-:|:-:|:-:\n",
    "$w_0$|$x_{01}$|$x_{02}$\n",
    "$w_1$|$x_{11}$|$x_{12}$\n",
    "$w_2$|$x_{21}$|$x_{22}$\n",
    "\n",
    "$x_{ij}$ 表示词 $w_i$ 的标签是 $l_j$ 。<br/>\n",
    "同时，CRF层的转移概率矩阵如下:\n",
    "\n",
    "|$l_1$|$l_2$\n",
    ":-:|:-:|:-:\n",
    "$l_1$|$t_{11}$|$t_{12}$\n",
    "$l_2$|$t_{21}$|$t_{22}$\n",
    "\n",
    "$t_{ij}$ 是标签 $i$ 到标签 $j$ 的转移概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3 开始战斗\n",
    "**记住目标：**$log(e^{ S_ {1}} + e^{ S_ {2}} + \\ldots + e^{S_{N}})$<br/>\n",
    "\n",
    "******\n",
    "这是一个累计求和的过程。思路和动态编程相类似。简单地说，计算 $w_0$ 所有可能路径的总分。然后，使用总分来计算 $w_0\\rightarrow w_1$ 的总分。最后，使用上一步总分来计算 $w_0\\rightarrow w_1 \\rightarrow w_2 $ 的总分。该总分就是我们需要的总分。<br/>\n",
    "在下面步骤中，可以看到两个变量: ***obs*** 和 ***previous***,***previous*** 保存前一步的最终结果，***obs*** 表示当前词汇的信息。\n",
    "******\n",
    "$w_0$ :\n",
    "\n",
    "> $obs=[x_{01},x_{02}]$ <br/>\n",
    "> $previous=None$\n",
    "\n",
    "如果句子只有1个词汇 $w_0$，没有前一步的结果，则 *previous* 为None。此外，我们只看第1个词汇 $obs=[x_{01},x_{02}]$ ，其中 $x_{01}和x_{02}$ 是上面提到的发射概率。<br/>\n",
    "你可能想，$w_0$ 的所有可能路径的总分是多少？结果是:\n",
    "> $TotalScore(w_0)=log(e^{x_{01}}+e^{x_{02}})$\n",
    "\n",
    "******\n",
    "$w_0 \\rightarrow w_1$:\n",
    ">$obs=[x_{11},x_{12}]$ <br/>\n",
    "> $previous=[x_{01},x_{02}]$\n",
    "\n",
    "1) 扩展变量 *previous* :\n",
    "> $ previous=\\left( \\begin{matrix} x_{01} & x_{01} \\\\ x_{02} &x_{02} \\end{matrix} \\right) $\n",
    "  \n",
    "2) 扩展 *obs* :\n",
    "> $ obs=\\left( \\begin{matrix} x_{11} & x_{12} \\\\ x_{11} &x_{12} \\end{matrix} \\right) $\n",
    "\n",
    "你可能会有疑惑，为何扩展 *previous* 和 *obs* 成为矩阵。因为矩阵可以有效计算总分，你可以在接下来的过程中看到。<br/>\n",
    "3) 对 *previous* 、*obs*和转移概率矩阵求和:\n",
    "> $scores=\\left( \\begin{matrix} x_{01} & x_{01} \\\\ x_{02} &x_{02} \\end{matrix}\\right)+ \n",
    "          \\left( \\begin{matrix} x_{11} & x_{12} \\\\ x_{11} &x_{12} \\end{matrix}\\right)+\n",
    "          \\left( \\begin{matrix} t_{11} & t_{12} \\\\ t_{21} &t_{22} \\end{matrix}\\right)\n",
    "          $ <br/>\n",
    "<br/>\n",
    ">$scores=\\left( \\begin{matrix} x_{01}+x_{11}+t_{11} & x_{01}+x_{12}+t_{12} \\\\ \n",
    "         x_{02}+x_{11}+t_{21} &x_{02}+x_{12}+t_{22} \\end{matrix}\\right)$\n",
    "         \n",
    "更新下一步 *previous* 的值变为:\n",
    "<img src=\"http://localhost:9999/files/my_learning/pic/crf_totalscore_1.png\" width = \"500\" height = \"100\" alt=\"ctf\" align=center/>\n",
    "实际上，第2次迭代已经结束。有些人可能想知道怎么计算从 $w_0$ 到 $w_1$ 的所有路径( $label_1\\rightarrow label_1$,$label_1\\rightarrow label_2,label_2\\rightarrow label_1,label_2\\rightarrow label_2$ )的总分。<br/>\n",
    "我们使用更新后的 *previous* 的元素:<br/>\n",
    "<img src=\"http://localhost:9999/files/my_learning/pic/crf_totalscore_2.png\" width = \"500\" height = \"100\" alt=\"ctf\" align=center/>\n",
    "\n",
    "你会发现这就是我们的目标 $log(e^{ S_ {1}} + e^{ S_ {2}} + \\ldots + e^{S_{N}})$ 。<br/>\n",
    "从等式中可知:\n",
    "\n",
    "- $S_1=x_{01}+x_{11}+t_{11} (label_1\\rightarrow label_1)$ \n",
    "- $S_2=x_{02}+x_{11}+t_{21} (label_2\\rightarrow label_1)$ \n",
    "- $S_3=x_{01}+x_{12}+t_{12} (label_1\\rightarrow label_2)$ \n",
    "- $S_4=x_{02}+x_{12}+t_{22} (label_2\\rightarrow label_2)$ \n",
    "\n",
    "******\n",
    "$w_0 \\rightarrow w_1 \\rightarrow w_2$:<br/>\n",
    "实际上，这次迭代和上次迭代一样。<br/>\n",
    "<img src=\"http://localhost:9999/files/pic/crf_totalscore_3.png\" width = \"600\" height = \"100\" alt=\"ctf\" align=center />\n",
    "1) 扩展变量 *previous* :\n",
    "<img src=\"http://localhost:9999/files/pic/crf_totalscore_4.png\" width = \"600\" height = \"100\" alt=\"ctf\" align=center />\n",
    "2) 扩展 *obs* :\n",
    "> $ obs=\\left( \\begin{matrix} x_{21} & x_{22} \\\\ x_{21} &x_{22} \\end{matrix} \\right) $\n",
    "\n",
    "3) 对 *previous* 、*obs*和转移概率矩阵求和:\n",
    "<img src=\"http://localhost:9999/files/pic/crf_totalscore_5.png\" width = \"800\" height = \"150\" alt=\"ctf\" align=center />\n",
    "即:\n",
    "<img src=\"http://localhost:9999/files/pic/crf_totalscore_6.png\" width = \"800\" height = \"150\" alt=\"ctf\" align=center />\n",
    "更新下一步 *previous* 的值变为:\n",
    "<img src=\"http://localhost:9999/files/pic/crf_totalscore_7.png\" width = \"800\" height = \"150\" alt=\"ctf\" align=center />\n",
    "在最后一次迭代中，我们使用更新后的 *previous* 的元素计算总分:\n",
    "<img src=\"http://localhost:9999/files/pic/crf_totalscore_8.png\" width = \"600\" height = \"200\" alt=\"ctf\" align=center />\n",
    "我们达到了计算$log(e^{ S_ {1}} + e^{ S_ {2}} + \\ldots + e^{S_{N}})$的目的。在我们的迷你版句子中，有3个词汇和2个标签，因此有8种可能路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  2.6 预测新句子的标签\n",
    "在上一节，我们学习了BiLSTM-CRF模型的结构和CRF的损失函数。你可以使用各种各样的开源框架(比如Keras, Chainer, TensorFlow等等)实现自己的BiLSTM-CRF模型。最重要的这些框架都自动实现了反向传播，因此在训练模型时你不用自己来实现反向传播。同时，一些框架已经实现了CRF层，只需增加一行代码就可以让自己的模型加入CRF层。<br/>\n",
    "这一节我们将展示训练好模型后怎样预测一个句子的标签。\n",
    "#### 2.6.1 从BiLSTM-CRF模型得到发射概率和转移概率\n",
    "假设我们的句子有3个词汇: $ \\boldsymbol x=[w_0,w_1,w_2] $ <br/>\n",
    "另外，我们从BiLSTM-CRF模型得到发射概率和转移概率，具体如下：\n",
    "\n",
    "|$l_1$|$l_2$\n",
    ":-:|:-:|:-:\n",
    "$w_0$|$x_{01}$|$x_{02}$\n",
    "$w_1$|$x_{11}$|$x_{12}$\n",
    "$w_2$|$x_{21}$|$x_{22}$\n",
    "\n",
    "$x_{ij}$ 表示词 $w_i$ 的标签是 $l_j$ 。<br/>\n",
    "\n",
    "|$l_1$|$l_2$\n",
    ":-:|:-:|:-:\n",
    "$l_1$|$t_{11}$|$t_{12}$\n",
    "$l_2$|$t_{21}$|$t_{22}$\n",
    "\n",
    "$t_{ij}$ 是标签 $i$ 到标签 $j$ 的转移概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2 开始预测\n",
    "如果你熟悉Viterbi算法，那这一节就很简单，但是不熟悉也没关系。和上一节一样，我会一步一步演示算法。我们会从句子的左边到右边来运行预测算法，具体如下：\n",
    "\n",
    "- $w_0$\n",
    "- $w_0\\rightarrow w_1$\n",
    "- $w_0\\rightarrow w_1\\rightarrow w_2$\n",
    "\n",
    "有两个变量: *obs* 和 *previous*, *previous* 保存上一步的最后结果，*obs*表示当前词汇的信息。<br/>\n",
    "$alpha_0$ 表示最好分值的历史数据，$alpha_1$ 表示相应索引的历史数据。这两个变量的细节在使用时说明。如下图所示，可以将两个变量表示为狗狗丛林冒险时留下的斑点，这些斑点可以帮助狗狗找到回家的路。\n",
    "<img src=\"http://localhost:9999/files/my_learning/pic/crf_dog.png\" width = \"400\" height = \"80\" alt=\"ctf\" align=center/>\n",
    "******\n",
    "$w_0$:\n",
    "> $obs=[x_{01},x_{02}]$ <br/>\n",
    "> $previous=None$\n",
    "\n",
    "当前，我们只观察到第一个词汇 $w_0$，直接就能得到$w_0$最好的标签。<br/>\n",
    "比如: 如果 $obs=[x_{01}=0.2,x_{02}=0.8]$，显然，$w_0$最好的标签是 $l_2$。<br/>\n",
    "因为只有一个词汇，标签之间没有转移，转移概率用不到。\n",
    "******\n",
    "$w_0\\rightarrow w_1$:\n",
    "> $obs=[x_{11},x_{12}]$\n",
    "> $previous=[x_{01},x_{02}]$\n",
    "\n",
    "1) 扩展 *previous*:\n",
    "> $previous=\\left( \\begin{matrix} previous[0] & previous[0] \\\\ previous[1] &previous[1] \\end{matrix}\\right)=\n",
    "          \\left( \\begin{matrix} x_{01} & x_{01} \\\\ x_{02} &x_{02} \\end{matrix}\\right)\n",
    "          $<br/>\n",
    " <br/>\n",
    "\n",
    "2) 扩展 *obs*:\n",
    "> $obs=\\left( \\begin{matrix} obs[0] & obs[1] \\\\ obs[0] &obs[1] \\end{matrix}\\right)=\n",
    "          \\left( \\begin{matrix} x_{11} & x_{12} \\\\ x_{11} &x_{12} \\end{matrix}\\right)\n",
    "          $\n",
    "\n",
    "3) 对*previous*、*obs*和转移概率矩阵加总求和:\n",
    "> $scores=\\left( \\begin{matrix} x_{01} & x_{01} \\\\ x_{02} &x_{02} \\end{matrix}\\right)+ \n",
    "          \\left( \\begin{matrix} x_{11} & x_{12} \\\\ x_{11} &x_{12} \\end{matrix}\\right)+\n",
    "          \\left( \\begin{matrix} t_{11} & t_{12} \\\\ t_{21} &t_{22} \\end{matrix}\\right)\n",
    "          $\n",
    "\n",
    "则:\n",
    "> $scores=\\left( \\begin{matrix} x_{01}+x_{11}+t_{11} & x_{01}+x_{12}+t_{12} \\\\ \n",
    "           x_{02}+x_{11}+t_{21} &x_{02}+x_{12}+t_{22} \\end{matrix}\\right) $\n",
    "\n",
    "你会觉得这和上一节计算所有可能路径的总分过程没什么区别。别急，你会很快发现不同。<br/>\n",
    "更新下一次迭代的 *previous* 值为：\n",
    "> $previous=[max(scores_{00},scores_{10}),max(scores_{01},scores_{11})]$\n",
    "\n",
    "比如，我们的得分是:\n",
    ">$scores=\\left( \\begin{matrix} x_{01}+x_{11}+t_{11} & x_{01}+x_{12}+t_{12} \\\\ \n",
    "           x_{02}+x_{11}+t_{21} &x_{02}+x_{12}+t_{22} \\end{matrix}\\right)=\n",
    "         \\left( \\begin{matrix} 0.2 & 0.3 \\\\ 0.5 & 0.4 \\end{matrix}\\right)$\n",
    "         \n",
    "则下一次迭代的 *previous* 为:\n",
    ">$previous=[max(scores_{00},scores_{10}),max(scores_{01},scores_{11})]=[0.5,0.4]$\n",
    "\n",
    "这个 *previous* 列表保存着当前词汇对应每个标签的最大分值。<br/>\n",
    "**[Example Start]** <br/>\n",
    "比如: 我们有两个标签，$label1(l_1)和label2(l_2)$，这两个标签相应的索引为0和1(因为在程序中，索引从0开始)。<br/>\n",
    "$previous[0]$ 是以第0个标签 $l_1$ 结尾的路径的最大分值，类似地，$previous[1]$ 是以第1个标签 $l_2$ 结尾的路径的最大分值。换句话说，每次迭代只保留每个标签最好路径的信息($previous=[max(scores_{00},scores_{10}),max(scores_{01},scores_{11})]$)，丢掉分值较小的路径信息。<br/>\n",
    "**[Example End]** <br/>\n",
    "<br/>\n",
    "同时，有 $alpha_0$ 和 $alpha_1$ 这两个变量保存历史信息(得分和索引)。<br/>\n",
    "对于本次迭代，将最高分值保存到 $alpha_0$ ，为方便起见，每个标签的最大分值用下划线标注。\n",
    "> <img src=\"http://localhost:9999/files/my_learning/pic/crf_best_score.png\" width = \"300\" height = \"80\" alt=\"ctf\" align=center/>\n",
    "> $alpha_0=[(scores_{10},scores_{11})]=[(0.5,0.4)]$\n",
    "\n",
    "另外，将相应的行索引保存到 $alpha_1$ 里。\n",
    "> $alpha_1=[RowIndex(scores_{10}),RowIndex(scores_{11})=[(1,1)]$\n",
    "\n",
    "解释一下，$l_1$ 的索引是0，$l_2$的索引是1。因此，$(1,1)=(l_2,l_2)$ 表示，对于当前的词汇 $w_i$和标签 $l^{(i)}$:\n",
    "> (1,1) <br/>\n",
    ">$=(l_2,l_2)$ <br/>\n",
    ">$=我们可以得到最大分值0.5当路径是 l^{(i-1)}=l_2\\rightarrow l^{(i)}=l_1,$<br/>\n",
    ">$=我们可以得到最大分值0.4当路径是 l^{(i-1)}=l_2\\rightarrow l^{(i)}=l_2)$<br/>\n",
    "\n",
    "$l^{(i-1)}$ 是前一个词汇 $w_{i-1}$ 的标签。(批注:$alpha_1$列表的元素是个元组类型，该元组里面的元素代表上一步的标签索引，位置代表当前的标签索引。比如元组(1,1),索引为0的元素是1，表示上一步标签是 $l_2$,元组索引为0，表示当前标签是 $l_1$，路径是 $l_2\\rightarrow l_1$ 。)\n",
    "******\n",
    "$w_0\\rightarrow w_1\\rightarrow w_2 $:\n",
    "> $obs=[x_{21},x_{22}]$<br/>\n",
    "> $previous=[0.5,0.4]$\n",
    "\n",
    "1) 扩展 *previous*:\n",
    "> $previous=\\left( \\begin{matrix} previous[0] & previous[0] \\\\ previous[1] &previous[1] \\end{matrix}\\right)=\n",
    "          \\left( \\begin{matrix} 0.5 & 0.5 \\\\ 0.4 &0.4 \\end{matrix}\\right)\n",
    "          $<br/>\n",
    "\n",
    "2) 扩展 *obs*:\n",
    "> $obs=\\left( \\begin{matrix} obs[0] & obs[1] \\\\ obs[0] &obs[1] \\end{matrix}\\right)=\n",
    "          \\left( \\begin{matrix} x_{21} & x_{22} \\\\ x_{21} &x_{22} \\end{matrix}\\right)\n",
    "          $\n",
    "\n",
    "3) 对*previous*、*obs*和转移概率矩阵加总求和:\n",
    "> $scores=\\left( \\begin{matrix} 0.5 & 0.5 \\\\ 0.4 &0.4 \\end{matrix}\\right)+ \n",
    "          \\left( \\begin{matrix} x_{21} & x_{22} \\\\ x_{21} &x_{22} \\end{matrix}\\right)+\n",
    "          \\left( \\begin{matrix} t_{11} & t_{12} \\\\ t_{21} &t_{22} \\end{matrix}\\right)\n",
    "          $\n",
    "\n",
    "则:\n",
    ">$scores=\\left( \\begin{matrix} 0.5+x_{21}+t_{11} & 0.5+x_{22}+t_{12} \\\\ 0.4+x_{21}+t_{21} &0.4+x_{22}+t_{22} \\end{matrix}\\right)$\n",
    "\n",
    "更新下一次迭代的 *previous* 为:\n",
    ">$previous=[max(scores_{00},scores_{10}),max(scores_{01},scores_{11})]$\n",
    "\n",
    "本次迭代的scores为:\n",
    "<img src=\"http://localhost:9999/files/my_learning/pic/crf_score.png\" width = \"200\" height = \"80\" alt=\"ctf\" align=center/>\n",
    "则最新的*previous* 为:\n",
    ">$previous=[0.8,0.9]$\n",
    "\n",
    "实际上,$previous[0]$ 和 $previous[1]$ 之间较大者是最好预测路径的分值。<br/>\n",
    "同时，每个标签的最大分值和索引也会加入到 $alpha_0$ 和 $alpha_1$ 中:\n",
    "<img src=\"http://localhost:9999/files/my_learning/pic/crf_alpha.png\" width = \"300\" height = \"80\" alt=\"ctf\" align=center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.6.3 找出具有最高分值的最佳路径\n",
    "最后一步，根据 $alpha_0$ 和 $alpha_1$ 找出具有最高分值的路径。我们会从后往前核查每个元素。\n",
    "*****\n",
    "$w_1\\rightarrow w_2 $:<br/>\n",
    "首先，查看 $alpha_0$ 和 $alpha_1$ 的最后元素: (0.8,0.9)和(1,0)。0.9表示，当标签为 $l_2$ 时，可以得到路径的最高分值0.9。$l_2$ 的索引是1，因此得到(1,0)[1]=0。索引\"0\"表示前一个标签是 $l_1$($l_1$的索引是0)。故可以得到 $w_1\\rightarrow w_2$ 的最佳路径是$l_1\\rightarrow l_2 $。<br/>\n",
    "$w_0\\rightarrow w_1 $:<br/>\n",
    "其次，我们继续往前推，得到 $alpha_1$ 的元素: (1,1)。从上一段内容我们知道 $w_1$ 的标签是 $l_1$(索引是0)，因此得到(1,1)[0]=1。故可以得到 $w_0\\rightarrow w_1$ 的最佳路径是$l_2\\rightarrow l_1 $。<br/>\n",
    "恭喜，例子的最佳路径是: $l_2\\rightarrow l_1\\rightarrow l_2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Pytorch实现\n",
    "本节讲解用Pytorch实现一个简单的BiLSTM-CRF模型。\n",
    "### 3.1 构建一个迷你的训练数据\n",
    "假设有10个句子，标签类型有3个，额外增加两个标签:\"START\"和\"END\"，即：\n",
    "\n",
    "Lable|Index\n",
    "--- | ---\n",
    "B|0\n",
    "I|1\n",
    "O|2\n",
    "START|3\n",
    "END|4\n",
    "\n",
    "为句子中每个词汇随机分配词汇索引id,随机分配标签id。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练数据语料\n",
    "sentence_num=5 \n",
    "training_data = [(\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ")]*sentence_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建词汇的索引\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#处理句子，得到词汇的索引组成的向量\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    "\n",
    "sentence_demo=prepare_sequence(training_data[0][0],word_to_ix)\n",
    "sentence_demo.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#处理标签，得到标签索引组成的向量\n",
    "tags_demo = torch.LongTensor([tag_to_ix[t] for t in training_data[0][1]])\n",
    "tags_demo.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 词汇embedding和BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 5 #词汇embedding的特征大小\n",
    "hidden_dim = 4 #BiLSTM隐状态的特征维度\n",
    "vocab_size = len(word_to_ix) #词汇数量\n",
    "tagset_size = len(tag_to_ix) #标签数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#词汇embedding初始化，会随着模型训练而改变参数，可以将预训练的词汇embedding加载进来\n",
    "word_embeds = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#句子的向量化表示\n",
    "embeds = word_embeds(sentence_demo).view(len(sentence_demo), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建单层的BiLSTM模型\n",
    "lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#在用BiLSTM模型前向计算时，需要先初始化两个隐状态的特征向量\n",
    "def init_hidden():\n",
    "    return (Variable(torch.randn(2, 1, hidden_dim // 2)),\n",
    "            Variable(torch.randn(2, 1, hidden_dim // 2)))\n",
    "hidden = init_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用BiLSTM模型前向计算\n",
    "lstm_out, hidden = lstm(embeds, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将lstm_out进行reshape\n",
    "lstm_out = lstm_out.view(len(sentence_demo), hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#通过线性函数将BiLSTM模型的输出映射到发射概率\n",
    "hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "emission_data = hidden2tag(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 初始化转移概率矩阵\n",
    "转移概率矩阵size是标签size，再加上两个标签:\"START\"和\"END\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始转移概率矩阵\n",
    "transitions = nn.Parameter(\n",
    "            torch.randn(tagset_size+2, tagset_size+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_index=tagset_size #START索引\n",
    "end_index=tagset_size+1 #END索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 给发射概率矩阵增加START和END概率\n",
    "BiLSTM模型的输出是没有标签\"START\"和\"END\"的发射概率，需要手工增加相应的发射概率(比如，$x_{start,START}=0$ 和 $x_{start,other label} =-1000$)。假如句子有3个词汇，则增加后的发射概率矩阵为:\n",
    "\n",
    "Word\\Lable|B|I|O|START|END\n",
    "--- | ---|--- | ---| ---|---\n",
    "start|-1000|-1000|-1000|0|-1000\n",
    "w0|0.1|0.2|0.6|-1000|-1000\n",
    "w1|0.38|0.18|0.99|-1000|-1000\n",
    "w2|0.87|0.66|0.53|-1000|-1000\n",
    "end|-1000|-1000|-1000|-1000|0\n",
    "\n",
    "注意:发射概率矩阵的标签顺序和转移概率矩阵的标签顺序需要保持一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small=-1000 #填充值\n",
    "def emission_pad(emission_data):\n",
    "    emission_size=emission_data.size() #发射概率矩阵的size\n",
    "    row_np = np.array([small]  * emission_size[1], \n",
    "                      dtype='f').reshape(-1, emission_size[1])\n",
    "    row_emission = Variable(torch.from_numpy(row_np), volatile=False)\n",
    "    column_np = np.array([small] *  (emission_size[0] + 2), \n",
    "                    dtype='f').reshape( (emission_size[0] + 2),-1)\n",
    "    column_emission = Variable(torch.from_numpy(column_np), volatile=False)\n",
    "    emission_score=torch.cat(\n",
    "            (torch.cat((row_emission, emission_data, row_emission), dim=0), \n",
    "            column_emission, column_emission), dim=1)\n",
    "    emission_score.data[0,start_index]=0\n",
    "    emission_score.data[-1,end_index]=0\n",
    "    return emission_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emission_score=emission_pad(emission_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 给标签索引向量增加START和END\n",
    "标签索引向量增加\"START\"和\"END\"，便于后面处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_pad(label_data):\n",
    "    return torch.cat((torch.LongTensor([start_index]),label_data,\n",
    "           torch.LongTensor([end_index])),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_demo_pad=label_pad(tags_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 计算真实路径的发射概率之和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_true_emission_score(emission_data, label_data):\n",
    "    return torch.sum(emission_data[list(range(emission_data.size()[0])), label_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.3632\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_true_emission_score(emission_score,tags_demo_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 计算真实路径的转移概率之和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_true_transition_score(label_data):\n",
    "    return torch.sum(transitions[label_data[:-1], label_data[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3.4474\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_true_transition_score(tags_demo_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 计算全部路径的log分值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算exp加和的log值\n",
    "def log_sum_exp(input_data,dim=0):\n",
    "    max_scores, _ = input_data.max(dim=dim, keepdim=False)\n",
    "    output = input_data - max_scores.expand_as(input_data)\n",
    "    return max_scores + torch.log(torch.sum(torch.exp(output),\n",
    "                                    dim=dim, keepdim=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算全部路径的log分值\n",
    "def get_all_possible_path_score(emission):\n",
    "    previous=emission[0]\n",
    "    for obs in emission[1:]:\n",
    "        previous=previous.view(-1,1).expand(tagset_size+2,tagset_size+2)\n",
    "        obs=obs.view(1,-1).expand(tagset_size+2,tagset_size+2)\n",
    "        scores=previous+obs+transitions\n",
    "        previous=log_sum_exp(scores,dim=0)\n",
    "\n",
    "    return log_sum_exp(previous,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 13.4412\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_possible_path_score(emission_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 计算损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(emission_data,label_data):\n",
    "    #增加START和END标签的发射概率和标签序列值\n",
    "    emission_data=emission_pad(emission_data)\n",
    "    label_data=label_pad(label_data)\n",
    "    #计算真实路径的发射概率之和\n",
    "    true_emission_score=get_sentence_true_emission_score(emission_data,label_data)\n",
    "    #计算真实路径的转移概率之和\n",
    "    true_transition_score=get_sentence_true_transition_score(label_data)\n",
    "    #计算全部路径的log分值\n",
    "    all_possible_path_score=get_all_possible_path_score(emission_data)\n",
    "    #计算损失函数\n",
    "    return -(true_emission_score+true_transition_score-all_possible_path_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 10.3570\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(emission_data,tags_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 viterbi预测路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_decode(emission_data):\n",
    "    emission=emission_pad(emission_data)\n",
    "    previous = emission[0]\n",
    "    alpha0=[]\n",
    "    alpha1 = []\n",
    "    for obs in emission[1:]:\n",
    "        previous = previous.view(-1, 1).expand(tagset_size + 2, tagset_size + 2)\n",
    "        obs = obs.view(1, -1).expand(tagset_size + 2, tagset_size + 2)\n",
    "        scores = previous + obs + transitions\n",
    "        previous,index=scores.max(dim=0,keepdim=True)\n",
    "        alpha0.append(previous.data.tolist()[0])\n",
    "        alpha1.append(index.data.tolist()[0])\n",
    "    initial_beta=alpha0[-1].index(max(alpha0[-1]))\n",
    "    sequence=[initial_beta]\n",
    "    for item in alpha1[::-1]:\n",
    "        initial_beta=item[initial_beta]\n",
    "        sequence.append(initial_beta)\n",
    "    sequence=sequence[::-1][1:-1]\n",
    "    return max(alpha0[-1]),sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.36688232421875, [1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_decode(emission_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 组装CRF模型\n",
    "将前面的CRF模型各个零部件组装起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    def __init__(self, label_size, is_cuda=False):\n",
    "        super().__init__()\n",
    "        self.label_size = label_size #标签数量\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(label_size+2, label_size+2)) #初始化转移概率矩阵\n",
    "        self.start_index=label_size #START索引\n",
    "        self.end_index=label_size+1 #END索引\n",
    "        self._init_weight()\n",
    "        self.torch = torch.cuda if is_cuda else torch\n",
    "        self.small=-1000.0 #填充值\n",
    "\n",
    "    def _init_weight(self):\n",
    "        init.xavier_uniform(self.transitions)\n",
    "\n",
    "    #处理发射概率矩阵和标签矩阵\n",
    "    def emission_pad(self,emission_data):\n",
    "        emission_size=emission_data.size() #发射概率矩阵的size\n",
    "        row_np = np.array([self.small]  * emission_size[1], \n",
    "                          dtype='f').reshape(-1, emission_size[1])\n",
    "        row_emission = Variable(torch.from_numpy(row_np), volatile=False)\n",
    "        column_np = np.array([self.small] *  (emission_size[0] + 2), \n",
    "                        dtype='f').reshape( (emission_size[0] + 2),-1)\n",
    "        column_emission = Variable(torch.from_numpy(column_np), volatile=False)\n",
    "        emission_score=torch.cat(\n",
    "                (torch.cat((row_emission, emission_data, row_emission), dim=0), \n",
    "                column_emission, column_emission), dim=1)\n",
    "        emission_score.data[0,self.start_index]=0\n",
    "        emission_score.data[-1,self.end_index]=0\n",
    "        return emission_score\n",
    "\n",
    "    def label_pad(self,label_data):\n",
    "        return torch.cat((torch.LongTensor([self.start_index]),label_data,\n",
    "           torch.LongTensor([self.end_index])),0)\n",
    "\n",
    "    # 获取真实路径的发射概率之和\n",
    "    def get_sentence_true_emission_score(self,emission_data, label_data):\n",
    "        return torch.sum(emission_data[list(range(emission_data.size()[0])), label_data])\n",
    "\n",
    "    #获取真实路径的转移概率之和\n",
    "    def get_sentence_true_transition_score(self, label_data):\n",
    "        return torch.sum(self.transitions[label_data[:-1], label_data[1:]])\n",
    "\n",
    "    #获取全部路径的log分值\n",
    "    def get_all_possible_path_score(self,emission):\n",
    "        previous=emission[0]\n",
    "        for obs in emission[1:]:\n",
    "            previous=previous.view(-1,1).expand(self.label_size+2,self.label_size+2)\n",
    "            obs=obs.view(1,-1).expand(self.label_size+2,self.label_size+2)\n",
    "            scores=previous+obs+self.transitions\n",
    "            previous=self.log_sum_exp(scores,dim=0)\n",
    "\n",
    "        return self.log_sum_exp(previous,dim=0)\n",
    "\n",
    "    # viterbi预测路径\n",
    "    def viterbi_decode(self,emission):\n",
    "        emission=self.emission_pad(emission)\n",
    "        previous = emission[0]\n",
    "        alpha0=[]\n",
    "        alpha1 = []\n",
    "        for obs in emission[1:]:\n",
    "            previous = previous.view(-1, 1).expand(self.label_size + 2, self.label_size + 2)\n",
    "            obs = obs.view(1, -1).expand(self.label_size + 2, self.label_size + 2)\n",
    "            scores = previous + obs + self.transitions\n",
    "            previous,index=scores.max(dim=0,keepdim=True)\n",
    "            alpha0.append(previous.data.tolist()[0])\n",
    "            alpha1.append(index.data.tolist()[0])\n",
    "        initial_beta=alpha0[-1].index(max(alpha0[-1]))\n",
    "        sequence=[initial_beta]\n",
    "        for item in alpha1[::-1]:\n",
    "            initial_beta=item[initial_beta]\n",
    "            sequence.append(initial_beta)\n",
    "        sequence=sequence[::-1][1:-1]\n",
    "        return max(alpha0[-1]),sequence\n",
    "\n",
    "    def log_sum_exp(self,input_data,dim=0):\n",
    "        max_scores, _ = input_data.max(dim=dim, keepdim=False)\n",
    "        output = input_data - max_scores.expand_as(input_data)\n",
    "        return max_scores + torch.log(torch.sum(torch.exp(output), dim=dim, keepdim=False))\n",
    "    \n",
    "    #计算损失函数\n",
    "    def forward(self,emission_data,label_data):\n",
    "        emission_data=self.emission_pad(emission_data)\n",
    "        label_data=self.label_pad(label_data)\n",
    "        true_emission_score=self.get_sentence_true_emission_score(emission_data,label_data)\n",
    "        true_transition_score=self.get_sentence_true_transition_score(label_data)\n",
    "        all_possible_path_score=self.get_all_possible_path_score(emission_data)\n",
    "        return -(true_emission_score+true_transition_score-all_possible_path_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12 组装BiLSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim #词embedding的向量大小\n",
    "        self.hidden_dim = hidden_dim #BiLSTM的隐状态特征大小\n",
    "        self.vocab_size = vocab_size #词汇数量\n",
    "        self.tag_to_ix = tag_to_ix #标签索引\n",
    "        self.tagset_size = len(tag_to_ix) #标签数量\n",
    "        #词汇embedding初始化\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #BiLSTM模型初始化\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "        #将BiLSTM模型输出线性映射到标签\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        #CRF模型初始化\n",
    "        self.crf = CRF(self.tagset_size,is_cuda=False) \n",
    "    \n",
    "    #计算得到发射概率矩阵\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "    \n",
    "    #计算得到CRF损失函数\n",
    "    def neg_log_likelihood(self,sentence,label_data):\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        return self.crf(lstm_feats,label_data)\n",
    "    \n",
    "    #BiLSTM模型隐状态初始化\n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.randn(2, 1, self.hidden_dim // 2)),\n",
    "                Variable(torch.randn(2, 1, self.hidden_dim // 2)))\n",
    "    \n",
    "    #预测最大分值和对应的标签序列\n",
    "    def forward(self,sentence):\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        score,tag_seq=self.crf.viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, embedding_dim, hidden_dim)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 -> avg loss: 4.944132\n",
      "Epoch: 100 -> avg loss: 0.615695\n",
      "Epoch: 150 -> avg loss: 0.272659\n",
      "Epoch: 200 -> avg loss: 0.174748\n",
      "Epoch: 250 -> avg loss: 0.128589\n",
      "Epoch: 300 -> avg loss: 0.101991\n",
      "Epoch: 350 -> avg loss: 0.083527\n",
      "Epoch: 400 -> avg loss: 0.071354\n",
      "Epoch: 450 -> avg loss: 0.061878\n",
      "Epoch: 500 -> avg loss: 0.053861\n",
      "Epoch: 550 -> avg loss: 0.048413\n",
      "Epoch: 600 -> avg loss: 0.044349\n",
      "Epoch: 650 -> avg loss: 0.039914\n",
      "Epoch: 700 -> avg loss: 0.036504\n",
      "Epoch: 750 -> avg loss: 0.033837\n",
      "Epoch: 800 -> avg loss: 0.031596\n",
      "Epoch: 850 -> avg loss: 0.029099\n",
      "Epoch: 900 -> avg loss: 0.027432\n",
      "Epoch: 950 -> avg loss: 0.025947\n",
      "Epoch: 1000 -> avg loss: 0.024589\n"
     ]
    }
   ],
   "source": [
    "loss_sum=0\n",
    "for epoch in range(1,1001): \n",
    "    for sentence, tags in training_data:\n",
    "        #第1步: 将pytorch的梯度置为0\n",
    "        model.zero_grad()\n",
    "        #第2步:准备模型的输入\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.LongTensor([tag_to_ix[t] for t in tags])\n",
    "        #第3步:前向传播计算损失函数\n",
    "        neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n",
    "        #第4步:计算梯度并更新参数\n",
    "        neg_log_likelihood.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum+=neg_log_likelihood.data[0]\n",
    "    if epoch%50==0:\n",
    "        print(\"Epoch: %d -> avg loss: %f\"%(epoch,loss_sum/(50*len(sentence))))\n",
    "        loss_sum=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.14 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实标签序列:\n",
      "[0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2]\n",
      "预测标签序列:\n",
      "[0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "precheck_tags = [tag_to_ix[t] for t in training_data[0][1]]\n",
    "print(\"真实标签序列:\")\n",
    "print(precheck_tags)\n",
    "\n",
    "print(\"预测标签序列:\")\n",
    "print(model(precheck_sent)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 进一步\n",
    "上述实现的BiLSTM-CRF是一条条数据进行训练，当训练数据比较大，可以采用批量训练的方法，只需对批量数据得到的损失函数求平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
