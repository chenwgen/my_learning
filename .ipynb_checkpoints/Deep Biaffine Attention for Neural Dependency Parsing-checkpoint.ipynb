{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Biaffine Attention for Neural Dependency Parsing\n",
    "这是斯坦福博士生Dozat提出的一种基于图的依存句法分析方法，主要解决两个问题：1、哪两个节点连依存弧；2、弧的标签是什么。具体文章见https://openreview.net/pdf?id=Hk95PK9le 和 https://web.stanford.edu/~tdozat/files/TDozat-CoNLL2017-Paper.pdf 。\n",
    "\n",
    "### 一、依存句法分析\n",
    "描述语法有两种主流观点：\n",
    " - (1) 短语结构文法。用固定数量的rule分解句子为短语和单词、分解短语为更短的短语或单词。一个取自WSJ语料库的短语结构树示例：\n",
    "<img src=\"http://localhost:9999/files/my_learning/pic/deep_biaffine_parser_wsj.jpg\" width=\"400\" height=\"300\" />\n",
    " - (2) 依存结构。用单词之间的依存关系来表达语法。箭头的尾部是dependent（被修饰的主题），箭头指向的是head(修饰语)。\n",
    " <img src=\"http://localhost:9999/files/my_learning/pic/deep_biaffine_parser_universal_ dependency.jpg\" width=\"500\" height=\"400\" />\n",
    " \n",
    " 依存句法分析有几个条件:\n",
    " - ROOT只能被一个词依赖。\n",
    " - 无环\n",
    " \n",
    " 依存句法分析有两种方法：\n",
    " - Transition-base parsing.\n",
    " - Graph-base parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、Deep Biaffine Parser结构\n",
    "#### 1. 训练数据格式\n",
    "依存句法分析的训练数据一般为CoNLL格式的语料，以.conll结尾。CONLL标注格式包含10列，分别为：<br/>\n",
    "********\n",
    "ID   FORM    LEMMA   CPOSTAG POSTAG  FEATS   HEAD    DEPREL  PHEAD   PDEPREL\n",
    "********\n",
    "前8列的含义如下：\n",
    "\n",
    " 序列号|类型|含义\n",
    " --|--|--\n",
    " 1|ID|当前词在句子中的序号\n",
    " 2|FORM|当前词语或标点\n",
    " 3|LEMMA|当前词语（或标点）的原型或词干，一般此列与FORM相同\n",
    " 4|CPOSTAG|当前词语的词性（粗粒度）\n",
    " 5|POSTAG|当前词语的词性（细粒度）\n",
    " 6|FEATS|句法特征\n",
    " 7|HEAD|当前词语的中心词\n",
    " 8|DEPREL|当前词语与中心词的依存关系\n",
    "\n",
    "依存句法分析使用的数据包括:第1列、第2列、第4列、第7列和第8列。\n",
    "\n",
    "#### 2. 模型输入数据\n",
    "依存句法分析的输入数据包括:\n",
    "- 词语。可以加载预训练的词向量或字符向量。\n",
    "- 词性。词性经过词性标注模型得到。\n",
    "\n",
    "#### 3. 输入层 (MLP)\n",
    "一个句子的词语和词性经过embedding，输入到(多层)BiLSTM,然后每个词汇的BiLSTM输出结果通过4个不同的ReLU(MLP)层得到4种特殊的向量表示：\n",
    "\n",
    "- $h^{arc-dep}$ : 该词作为dependent(子节点)寻找head(父节点)。\n",
    "- $h^{arc-head}$ : 该词作为head寻找其所有的dependents 。\n",
    "- $h^{rel-dep}$ : 该词作为dependent决定其label(弧标签)。\n",
    "- $h^{rel-head}$ : 该词作为head决定其所有dependents的label。\n",
    "\n",
    "输入层的结构如下:\n",
    "<img src=\"http://localhost:9999/files/my_learning/pic/deep_biaffine_parser_emb.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "假设一个句子有n个词汇，给定n个词汇的embedding向量$(v_{1}^{word},...,v_{n}^{word})$和相应的词性$(v_{1}^{tag},...,v_{n}^{tag})$,将两个向量列表成对拼接起来:<br/>\n",
    "> $x_{i} = v_{i}^{word} \\bigoplus v_{i}^{tag}$\n",
    "\n",
    "将它们输入到以$r_{0}$为初始化向量的BiLSTM结构,获得每个词的输出向量:<br/>\n",
    "> $r_{i} = BiLSTM(r_{0} , (x_{1} , ... , x_{n}))_{i}$ <br/>\n",
    "> $ h_{i},c_{i} = split(r_{i})$\n",
    "\n",
    "然后通过4个独立的MLP获得上述4种向量表示:\n",
    "> $h_{i}^{(arc-dep)} = MLP^{(arc-dep)}(h_{i})$ <br/>\n",
    "> $h_{i}^{(arc-head)} = MLP^{(arc-head)}(h_{i})$ <br/>\n",
    "> $h_{i}^{(rel-dep)} = MLP^{(rel-dep)}(h_{i})$ <br/>\n",
    "> $h_{i}^{(rel-head)} = MLP^{(rel-head)}(h_{i})$ <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 预测弧\n",
    "对于第 $i$ 个词，其它所有词作为其head的分数为：<br/>\n",
    "> $s_{i}^{(arc)}=H^{(arc-head)}W^{(arc)}h_{i}^{(arc-dep)}+H^{(arc-head)}b^{T(arc)}$ <br/>\n",
    "<br/>\n",
    "> $y_{i}^{'(arc)}=arg \\underset{j}{max} s_{ij}^{(arc)}$ <br/>\n",
    "\n",
    "假设MLP得到的4个向量长度均为 $h$ ,则 $H^{(arc-head)}$ 的维度为 $n\\times h$ ,参数 $W^{(arc)}$ 维度为 $h\\times h$ ,偏置参数向量 $b^{T(arc)}$ 的维度为 $h\\times 1$,因此 $s_{i}^{arc}$ 的维度为 $n\\times 1$ ，维度具体计算过程如下:\n",
    "> $(n\\times 1)=(n\\times h)*(h\\times h)*(h\\times 1)+(n\\times h)*(h\\times 1)$\n",
    "\n",
    "将偏置合并到 $W$,可以用以下结构表示：\n",
    "<img src=\"http://localhost:9999/files/my_learning/pic/deep_biaffine_parser_arc.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "如果将词向量长度为 $h$ 的句子中 $n$ 个词放在一起，组成 $n\\times h$ 的矩阵$H^{(arc-dep)}$，则其他所有词的head的分数为:<br/>\n",
    "> $S^{(arc)}=H^{(arc-head)}(W^{(arc)}\\oplus b^{(arc)})(H^{(arc-dep)} \\oplus 1)^{T}$ <br/>\n",
    "\n",
    "对应的维度是:\n",
    "> $ (n\\times n)=(n\\times h)*(h\\times (h+1))*((h+1)\\times n)$\n",
    "\n",
    "对于第 $i$ 个词，其它所有词作为其head的分数为矩阵 $S$ 第 $i$ 列的最大值：<br/>\n",
    ">$y_{i}^{'(arc)}=arg \\underset{j}{max} S_{ji}^{(arc)}$ <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 5.预测弧标签\n",
    "在确定了词 $i$ 的 $head$ 之后，使用另一个网络预测这条弧的label:\n",
    "> $s_{i}^{(rel)}=h_{y_{i}^{'(arc)}}^{T(rel-head)}U^{(rel)}h_{i}^{(rel-dep)}+W^{(rel)}(h_{i}^{(rel-dep)}\\oplus h_{y_{i}^{'(arc)}}^{(rel-head)})+b^{(rel)} $ <br/>\n",
    "><br/>\n",
    "> $y_{i}^{'(rel)}=arg \\underset{j}{max} s_{ij}^{(rel)}$ <br/>\n",
    "\n",
    "若 $label$ 有 $r$ 种标签，$h_{y_{i}^{'(arc)}}^{T(rel-head)}$ 和 $h_{i}^{(rel-dep)}$ 的维度是 $h \\times 1$, $U^{(rel)}$ 的维度是 $r \\times h \\times h$ , $W^{(rel)}$ 的维度是 $r \\times 2h$ , $b$ 维度是 $r \\times 1$ ,则维度计算过程如下:\n",
    "> $(r \\times 1) = (1 \\times h)(r \\times h \\times h)(h \\times 1)+(r \\times 1\\times 2h)(2h \\times 1)+(r \\times 1\\times 1)$\n",
    "\n",
    "将偏置合并到 $U$ ,可以用以下结构表示：\n",
    "<img src=\"http://localhost:9999/files/my_learning/pic/deep_biaffine_parser_arc_label.jpg\" width=\"600\" height=\"400\" />\n",
    "\n",
    "计算时将 $U$ 中的 $r$ 个 $(h+1) \\times (h+1)$ 矩阵分别与 $h_{y_{i}}^{(rel-arc)} \\oplus 1$ 和 $h_{i}^{(rel-dep)} \\oplus 1$ 进行矩阵运算。<br/>\n",
    "\n",
    "若同时计算句子 $n$ 个词汇所对应的弧标签，则可表示为:\n",
    "> $S^{(rel)}= (H_{y^{'(arc)}}^{(rel-head)} \\oplus 1)U^{(rel)}(H^{(rel-dep)} \\oplus 1)^{T} $ \n",
    "\n",
    "对应的维度是:\n",
    ">$ (r\\times n \\times n)=(n \\times (h+1))(r\\times (h+1) \\times (h+1))((h+1)\\times n) $\n",
    "\n",
    "则第 $i$ 个词的 $label$ 得分 ,可以先根据弧的 $head$ , 从 $S^{(rel)}$ 的第2维度(dim=1)找到第 $j$ 行，再从第1维度(dim=0)的 $r$ 个得分中找到最大值:\n",
    ">$ s_{i}^{'(rel)}=S^{(rel)}[: , y_{i}^{'(arc)},i] $ <br/>\n",
    "><br/>\n",
    "> $s_{i}^{(rel)}=arg \\underset{j}{max}s_{ij}^{(rel)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 6. 损失函数和预测\n",
    "训练时，以上述两个分类器的softmax交叉熵损失作为优化目标。在测试时，通过为每个可能的根迭代地解决环问题生成一棵符合约束的树，然后选择其中总分最高的。比如用Chu-Liu/Edmonds算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、Pytorch构建模型\n",
    "实际模型训练时，每次迭代采用batch_size样本进行模型训练。具体代码见https://github.com/daandouwe/biaffine-dependency-parser。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BiAffineParser(nn.Module):\n",
    "\n",
    "    def __init__(self, word_vocab_size, word_emb_dim,\n",
    "                 tag_vocab_size, tag_emb_dim, emb_dropout,\n",
    "                 lstm_hidden, lstm_num_layers, lstm_dropout,\n",
    "                 mlp_arc_hidden, mlp_lab_hidden, mlp_dropout,\n",
    "                 num_labels, criterion=nn.CrossEntropyLoss()):\n",
    "        super(BiAffineParser, self).__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_emb_dim, padding_idx=0)\n",
    "        self.tag_embedding = nn.Embedding(tag_vocab_size, tag_emb_dim, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(p=emb_dropout)\n",
    "\n",
    "        # LSTM\n",
    "        lstm_input = word_emb_dim + tag_emb_dim\n",
    "        self.lstm = nn.LSTM(input_size=lstm_input, hidden_size=lstm_hidden,\n",
    "                            num_layers=lstm_num_layers, batch_first=True,\n",
    "                            dropout=lstm_dropout, bidirectional=True)\n",
    "\n",
    "        # Arc MLPs\n",
    "        mlp_input = 2*lstm_hidden\n",
    "        self.arc_mlp_h = MLP(mlp_input, mlp_arc_hidden, 2, 'ReLU', mlp_dropout)\n",
    "        self.arc_mlp_d = MLP(mlp_input, mlp_arc_hidden, 2, 'ReLU', mlp_dropout)\n",
    "        # Label MLPs\n",
    "        self.lab_mlp_h = MLP(mlp_input, mlp_lab_hidden, 2, 'ReLU', mlp_dropout)\n",
    "        self.lab_mlp_d = MLP(mlp_input, mlp_lab_hidden, 2, 'ReLU', mlp_dropout)\n",
    "\n",
    "        # BiAffine layers\n",
    "        self.arc_biaffine = BiAffine(mlp_arc_hidden, 1)\n",
    "        self.lab_biaffine = BiAffine(mlp_lab_hidden, num_labels)\n",
    "\n",
    "        # Loss criterion\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, words, tags):\n",
    "        \"\"\"\n",
    "        Compute the score matrices for the arcs and labels.\n",
    "        \"\"\"\n",
    "        words = self.word_embedding(words)\n",
    "        tags = self.tag_embedding(tags)\n",
    "        x = torch.cat((words, tags), dim=-1)\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        h, _ = self.lstm(x)\n",
    "\n",
    "        arc_h = self.arc_mlp_h(h)\n",
    "        arc_d = self.arc_mlp_d(h)\n",
    "        lab_h = self.lab_mlp_h(h)\n",
    "        lab_d = self.lab_mlp_d(h)\n",
    "\n",
    "        S_arc = self.arc_biaffine(arc_h, arc_d)\n",
    "        S_lab = self.lab_biaffine(lab_h, lab_d)\n",
    "        return S_arc, S_lab\n",
    "\n",
    "    def arc_loss(self, S_arc, heads):\n",
    "        \"\"\"\n",
    "        Compute the loss for the arc predictions.\n",
    "        \"\"\"\n",
    "        S_arc = S_arc.transpose(-1, -2)                     # [batch, sent_len, sent_len]\n",
    "        S_arc = S_arc.contiguous().view(-1, S_arc.size(-1)) # [batch*sent_len, sent_len]\n",
    "        heads = heads.view(-1)                              # [batch*sent_len]\n",
    "        return self.criterion(S_arc, heads)\n",
    "\n",
    "    def lab_loss(self, S_lab, heads, labels):\n",
    "        \"\"\"\n",
    "        Compute the loss for the label predictions on the gold arcs (heads).\n",
    "        \"\"\"\n",
    "        heads = heads.unsqueeze(1).unsqueeze(2)             # [batch, 1, 1, sent_len]\n",
    "        heads = heads.expand(-1, S_lab.size(1), -1, -1)     # [batch, n_labels, 1, sent_len]\n",
    "        S_lab = torch.gather(S_lab, 2, heads).squeeze(2)    # [batch, n_labels, sent_len]\n",
    "        S_lab = S_lab.transpose(-1, -2)                     # [batch, sent_len, n_labels]\n",
    "        S_lab = S_lab.contiguous().view(-1, S_lab.size(-1)) # [batch*sent_len, n_labels]\n",
    "        labels = labels.view(-1)                            # [batch*sent_len]\n",
    "        return self.criterion(S_lab, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "class BiAffine(nn.Module):\n",
    "    \"\"\"\n",
    "    Biaffine attention layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(BiAffine, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.U = nn.Parameter(torch.FloatTensor(output_dim, input_dim, input_dim))\n",
    "        init.xavier_uniform(self.U)\n",
    "\n",
    "    def forward(self, Rh, Rd):\n",
    "        Rh = Rh.unsqueeze(1)\n",
    "        Rd = Rd.unsqueeze(1)\n",
    "        S = Rh.matmul(self.U).matmul(Rd.transpose(-1, -2))\n",
    "        # S = Rh @ self.U @ Rd.transpose(-1, -2)\n",
    "        return S.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
